<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>teaching on Ryan Safner</title>
    <link>https://ryansafner.com/categories/teaching/</link>
    <description>Recent content in teaching on Ryan Safner</description>
    <generator>Source Themes academia (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 18 Oct 2019 13:47:00 +0000</lastBuildDate>
    
	    <atom:link href="https://ryansafner.com/categories/teaching/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>If You&#39;re Going to Learn R, Learn the Tidyverse</title>
      <link>https://ryansafner.com/post/if-youre-going-to-learn-r-learn-the-tidyverse/</link>
      <pubDate>Fri, 18 Oct 2019 13:47:00 +0000</pubDate>
      
      <guid>https://ryansafner.com/post/if-youre-going-to-learn-r-learn-the-tidyverse/</guid>
      <description>&lt;p&gt;This is an opinionated post based on how I teach &lt;a href=&#34;http://metricsf19.classes.ryansafner.com&#34;&gt;my undergraduate
econometrics course&lt;/a&gt;. It will
not be for everybody. The title applies mostly to anyone who wants to do
&lt;a href=&#34;https://ryansafner.com/post/econometrics-data-science-and-causal-inference/&#34;&gt;data science or
econometrics&lt;/a&gt;
with R. This is the second time I have taught this course with R, and I
have changed it around in many ways that I think optimize the process
for students. In this post, I’ll cover just two major changes:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Learn R before the econometric/statistical theory&lt;/li&gt;
&lt;li&gt;Learn &lt;em&gt;tidyverse&lt;/em&gt; R, specifically&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The &lt;a href=&#34;http://ryansafner.com/courses/econ480&#34;&gt;last time I taught this
course&lt;/a&gt; (and my first exploration
with R), I did neither. Part of this was because I had just learned base
R a few months before the course began, and was still learning and
discovering new commands as the semester went on. I waited several weeks
to introduce R, starting instead with econometric theory and review, and
then interspersing bits of R as it became relevant for each task
(running regressions, making plots, changing variables, and various
different models). While it was fine for most students, there are some
changes that will make students’ lives easier.&lt;/p&gt;
&lt;p&gt;I am still partway through the course, so it remains to be seen if
students R skills are more developed by the end of the course (as they
wrap up their projects) because they learned a lot of R &lt;em&gt;first&lt;/em&gt;, versus
in bite-sized chunks spread out over the semester.&lt;/p&gt;
&lt;h2 id=&#34;tidyverse-and-opinionated-r&#34;&gt;&lt;code&gt;tidyverse&lt;/code&gt; and Opinionated R&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;The tidyverse is an opinionated &lt;a href=&#34;https://www.tidyverse.org/packages&#34;&gt;collection of R
packages&lt;/a&gt; designed for data
science. All packages share an underlying design philosophy, grammar,
and data structures. - &lt;a href=&#34;https://www.tidyverse.org/&#34;&gt;tidyverse.org&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;One of the common refrains about what is awesome/terrible about R is
that there are multiple ways to accomplish the same task. Here is where
the opinions begin, so I’ll give mine: Code written with tidyverse
packages simply looks a lot better and is far easier for &lt;em&gt;humans&lt;/em&gt; to
read, particularly if you follow the &lt;a href=&#34;https://style.tidyverse.org/&#34;&gt;style
guidelines&lt;/a&gt;, as I do.&lt;/p&gt;
&lt;p&gt;For example, the following code takes data from the excellent
&lt;a href=&#34;https://github.com/jennybc/gapminder&#34;&gt;gapminder&lt;/a&gt; dataset and package,
and subsets the data to:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;look only at U.S. observations&lt;/li&gt;
&lt;li&gt;keep only the &lt;code&gt;year&lt;/code&gt;, &lt;code&gt;gdpPercap&lt;/code&gt; and &lt;code&gt;pop&lt;/code&gt; variables&lt;/li&gt;
&lt;li&gt;create a new variable called &lt;code&gt;GDP&lt;/code&gt; by multiplying &lt;code&gt;gdpPercap&lt;/code&gt; and
&lt;code&gt;pop&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;pre&gt;&lt;code&gt;gapminder1&amp;lt;-gapminder[gapminder$country==&amp;quot;United States&amp;quot;, c(&amp;quot;year&amp;quot;, &amp;quot;gdpPercap&amp;quot;, &amp;quot;pop&amp;quot;)]

gapminder1$gdp&amp;lt;-gapminder1$gdpPercap*gapminder1$pop
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is doable in base R, and often requires saving the output as a new
object for later use.&lt;/p&gt;
&lt;p&gt;Below, the same procedure is done with &lt;code&gt;dplyr&lt;/code&gt; and using the pipe &lt;code&gt;%&amp;gt;%&lt;/code&gt;
from &lt;code&gt;magrittr&lt;/code&gt; (both part of the &lt;code&gt;tidyverse&lt;/code&gt;).&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;gapminder %&amp;gt;%
  filter(country == &amp;quot;United States&amp;quot;) %&amp;gt;%
  select(year, gdpPercap, pop) %&amp;gt;%
  mutate(GDP=gdpPercap * pop)

## # A tibble: 12 x 4
##     year gdpPercap       pop     GDP
##    &amp;lt;int&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt;
##  1  1952    13990. 157553000 2.20e12
##  2  1957    14847. 171984000 2.55e12
##  3  1962    16173. 186538000 3.02e12
##  4  1967    19530. 198712000 3.88e12
##  5  1972    21806. 209896000 4.58e12
##  6  1977    24073. 220239000 5.30e12
##  7  1982    25010. 232187835 5.81e12
##  8  1987    29884. 242803533 7.26e12
##  9  1992    32004. 256894189 8.22e12
## 10  1997    35767. 272911760 9.76e12
## 11  2002    39097. 287675526 1.12e13
## 12  2007    42952. 301139947 1.29e13
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The beauty of &lt;code&gt;tidyverse&lt;/code&gt; (particularly &lt;code&gt;dplyr&lt;/code&gt;, which will be used the
most for data wrangling) comes from a few features:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;It uses active, common sense, natural language verbs to accomplish
most of its tasks. &lt;code&gt;filter&lt;/code&gt;, &lt;code&gt;select&lt;/code&gt;, and &lt;code&gt;mutate&lt;/code&gt; (among others)
are easy to understand what is happening at each stage.&lt;/li&gt;
&lt;li&gt;It allows use of the pipe &lt;code&gt;%&amp;gt;%&lt;/code&gt; to chain commands into a single
sequence (and better yet, every time I use a pipe, I start a new
line to make code far more readable).&lt;/li&gt;
&lt;li&gt;It &lt;em&gt;shows&lt;/em&gt; you the output by default and does not store (or
overwrite) it anywhere until or unless you assign it to an object.
This allows you to preview what your code does before you need to
worry about saving it.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Note that this can be done without use of the pipe, and by storing
objects, as such:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;gapminder_US&amp;lt;-filter(gapminder, country == &amp;quot;United States&amp;quot;)
gapminder_US&amp;lt;-select(gapminder_US, year, gdpPercap, pop)
gapminder_US&amp;lt;-mutate(gapminder_US, GDP=gdpPercap * pop)
gapminder_US

## # A tibble: 12 x 4
##     year gdpPercap       pop     GDP
##    &amp;lt;int&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt;
##  1  1952    13990. 157553000 2.20e12
##  2  1957    14847. 171984000 2.55e12
##  3  1962    16173. 186538000 3.02e12
##  4  1967    19530. 198712000 3.88e12
##  5  1972    21806. 209896000 4.58e12
##  6  1977    24073. 220239000 5.30e12
##  7  1982    25010. 232187835 5.81e12
##  8  1987    29884. 242803533 7.26e12
##  9  1992    32004. 256894189 8.22e12
## 10  1997    35767. 272911760 9.76e12
## 11  2002    39097. 287675526 1.12e13
## 12  2007    42952. 301139947 1.29e13
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;http://varianceexplained.org/r/teach-tidyverse/&#34;&gt;I am not alone in this
view.&lt;/a&gt; As far as it
relates to teaching, the implications are clear: The overwhelming
majority of students are new to “programming”, so they will be
frustrated &lt;em&gt;regardless&lt;/em&gt; of what order the content was taught, or in what
flavor of language. They do not need to know the “base R” way of doing
something just so that they can see that the tidyverse may be better or
more efficient - they just need to learn &lt;em&gt;one&lt;/em&gt; way to accomplish their
task, it might as well be (what I think is) the “better” one.&lt;/p&gt;
&lt;p&gt;So, I began the course (after two days of overview, why this course is
important, useful, etc.) with 4 intensive classes of learning R, and
tidyverse specifically. First, a day about base R, second, a day about
ggplot2 for data visualization, third, a day of data wrangling with
tidyverse, and finally a day about workflow and other tools (mainly r
markdown). I cover the basics behind each of them, and what I have
learned, below:&lt;/p&gt;
&lt;h2 id=&#34;class-1-is-base-r-necessary&#34;&gt;Class 1: Is Base R Necessary?&lt;/h2&gt;
&lt;p&gt;It somehow seems more “pure” to teach R from the ground up: First we
discuss basic R commands, then we build more complicated functions in,
then we show how to manipulate data, then we show how to plot, and
later: “oh by the way there are these packages that do all of this more
elegantly and in half as many lines of code.” That is how I taught
econometrics last year.&lt;/p&gt;
&lt;p&gt;We need to remember this is a class in econometrics and data analysis
that &lt;em&gt;uses&lt;/em&gt; R, not a class in computer science or the R language. In
fact, people with a computer science/programming background seem to find
R &lt;a href=&#34;https://www.youtube.com/watch?v=6S9r_YbqHy8&amp;amp;feature=youtu.be&#34;&gt;particularly annoying as a programming
language&lt;/a&gt;.
It is highly domain-specific (that domain chiefly being statistics), and
should be appreciated as such.&lt;/p&gt;
&lt;p&gt;In any case, I still made a point this year to make &lt;a href=&#34;https://metricsf19.classes.ryansafner.com/class/03-class/&#34;&gt;my first R-based
class&lt;/a&gt;
entirely about Base R without the bells and whistles.^[Though I allude to more complex examples to give them a taste of
what’s to come!] Students and
R-users need to understand some basic syntax of functions, as well as
the object-oriented nature of the language. I made sure that they
understand the following really well:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;What are packages, how to find, install, and load them.&lt;/li&gt;
&lt;li&gt;How to get help in R and on the internet for R functions.&lt;/li&gt;
&lt;li&gt;Different types of objects: especially &lt;code&gt;vector&lt;/code&gt; and &lt;code&gt;data.frame&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Different data classes: especially &lt;code&gt;numeric&lt;/code&gt; and &lt;code&gt;character&lt;/code&gt;, and
how to check them&lt;/li&gt;
&lt;li&gt;Some basic functions for making vectors and for getting statistics
(&lt;code&gt;c()&lt;/code&gt;, &lt;code&gt;mean()&lt;/code&gt;, &lt;code&gt;sd()&lt;/code&gt;, etc.)&lt;/li&gt;
&lt;li&gt;The basics of data frames: recognizing each column is a vector, how
to summarize them, how to subset by row, column, and element&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I try not to go overboard (I omit things like &lt;code&gt;factor&lt;/code&gt; and &lt;code&gt;logical&lt;/code&gt;
classes, &lt;code&gt;list&lt;/code&gt; or &lt;code&gt;matrix&lt;/code&gt; objects) and tell them not to worry too much
about manipulating dataframes in Base R, as that is what &lt;code&gt;tidyverse&lt;/code&gt;
will accomplish much more intuitively and efficiently.&lt;/p&gt;
&lt;h2 id=&#34;class-2-data-visualization-with-ggplot2&#34;&gt;Class 2: Data Visualization with &lt;code&gt;ggplot2&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;The &lt;a href=&#34;https://metricsf19.classes.ryansafner.com/class/04-class/&#34;&gt;second
R-class&lt;/a&gt; I
taught students all about data visualization with &lt;code&gt;ggplot2&lt;/code&gt;. Not only do
I think it is aesthetically superior to anything from Base R,^[If it was not clear in my posts by now, I have high aesthetic
standards.] it also
allows students to think about the important elements of a plot, and
optimize each one accordingly, with the “grammar of graphics.” This is a
pretty steep learning curve compared to typing &lt;code&gt;plot(y,x)&lt;/code&gt; and seeing a
scatterplot appear, but in the end, it is worth it.&lt;/p&gt;
&lt;p&gt;In class, I build a few plots layer by layer:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;data&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;aes&lt;/code&gt;thetics&lt;/li&gt;
&lt;li&gt;&lt;code&gt;geom&lt;/code&gt;etric objects&lt;/li&gt;
&lt;li&gt;&lt;code&gt;facets&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;labels&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;scale&lt;/code&gt;s&lt;/li&gt;
&lt;li&gt;&lt;code&gt;theme&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;At each layer, I explain what each layer does and many of the
possibilities for each layer.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ggplot(data = mpg)+
  aes(x = displ,
        y = hwy)+
  geom_point(aes(color = class))+
  geom_smooth()+
  facet_wrap(~year)+
  labs(x = &amp;quot;Engine Displacement (Liters)&amp;quot;,
       y = &amp;quot;Highway MPG&amp;quot;,
       title = &amp;quot;Car Mileage and Displacement&amp;quot;,
       subtitle = &amp;quot;More Displacement Lowers Highway MPG&amp;quot;,
       caption = &amp;quot;Source: EPA&amp;quot;,
       color = &amp;quot;Vehicle Class&amp;quot;)+
  scale_color_viridis_d()+
  theme_minimal()+
  theme(text = element_text(family = &amp;quot;Fira Sans&amp;quot;),
        legend.position=&amp;quot;bottom&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;unnamed-chunk-4-1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;This is also the students’ first exposure to &lt;code&gt;tidyverse&lt;/code&gt;, though it is
not yet apparent. My one regret, in retrospect, is that plot layers are
combined with &lt;code&gt;+&lt;/code&gt; instead of &lt;code&gt;%&amp;gt;%&lt;/code&gt;.^[I believe this is due to the unique history of &lt;code&gt;ggplot2&lt;/code&gt; coming
before the &lt;code&gt;tidyverse&lt;/code&gt; was a full idea. Though I hear in future
versions, this may be fixed!] After learning other &lt;code&gt;tidyverse&lt;/code&gt;
packages such as &lt;code&gt;dplyr&lt;/code&gt;, students would try to add plot layers with
&lt;code&gt;%&amp;gt;%&lt;/code&gt; but I would continuously have to remind them that layers are
combined with &lt;code&gt;+&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Recall, all of this is done well before we cover basic statistics or
econometric theory. When I was teaching students &lt;em&gt;how&lt;/em&gt; to construct
various plots with &lt;code&gt;ggplot2&lt;/code&gt;, this was before they knew &lt;em&gt;why&lt;/em&gt; they
needed a scatterplot or a boxplot.&lt;/p&gt;
&lt;h2 id=&#34;class-3-data-wrangling-with-tidyverse-mostly-dplyr&#34;&gt;Class 3: Data Wrangling with &lt;code&gt;tidyverse&lt;/code&gt; (mostly &lt;code&gt;dplyr&lt;/code&gt;)&lt;/h2&gt;
&lt;p&gt;The &lt;a href=&#34;https://metricsf19.classes.ryansafner.com/class/05-class/&#34;&gt;third
class&lt;/a&gt; was
all about &lt;code&gt;tidyverse&lt;/code&gt; as a unifying set of packages with a common
philosophy and grammar. I did discuss several core packages:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;tibble&lt;/code&gt; for friendlier dataframes^[I simply replaced all dataframes in the course with tibbles.]&lt;/li&gt;
&lt;li&gt;&lt;code&gt;magrittr&lt;/code&gt; for using the pipe &lt;code&gt;%&amp;gt;%&lt;/code&gt; to chain code together&lt;/li&gt;
&lt;li&gt;&lt;code&gt;readr&lt;/code&gt; for importing data (mostly &lt;code&gt;.csv&lt;/code&gt;^[And derivative packages such as &lt;code&gt;readxl&lt;/code&gt; and &lt;code&gt;haven&lt;/code&gt; for importing
other types of data such as &lt;code&gt;.xlsx&lt;/code&gt; or Stata’s &lt;code&gt;.dat&lt;/code&gt;.]&lt;/li&gt;
&lt;li&gt;&lt;code&gt;tidyr&lt;/code&gt; for reshaping data&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;But the primary focus was on &lt;code&gt;dplyr&lt;/code&gt; and its verbs:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;filter()&lt;/code&gt; to keep selected observations&lt;/li&gt;
&lt;li&gt;&lt;code&gt;select()&lt;/code&gt; to keep selected variables&lt;/li&gt;
&lt;li&gt;&lt;code&gt;arrange()&lt;/code&gt; to reorder observations by a value&lt;/li&gt;
&lt;li&gt;&lt;code&gt;mutate()&lt;/code&gt; to create new variables&lt;/li&gt;
&lt;li&gt;&lt;code&gt;summarize()&lt;/code&gt; to create summary statistics&lt;/li&gt;
&lt;li&gt;&lt;code&gt;group_by()&lt;/code&gt; for performing operations by group&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We worked with gapminder data to accomplish a wide variety of tasks
using these commands.&lt;/p&gt;
&lt;h2 id=&#34;class-4-optimizing-workflow-r-projects-and-markdown&#34;&gt;Class 4: Optimizing Workflow: R Projects and Markdown&lt;/h2&gt;
&lt;p&gt;The &lt;a href=&#34;https://metricsf19.classes.ryansafner.com/class/06-class/&#34;&gt;fourth and final
class&lt;/a&gt;
dedicated to R was all about optimizing workflow with a few tweaks. I
have not used Microsoft Office products in about 10 years (more posts to
come later), and in the last year, have migrated &lt;strong&gt;ALL&lt;/strong&gt; of my
document-preparation (that’s research papers, teaching materials, exams,
slides, websites, &lt;em&gt;everything&lt;/em&gt;) to markdown files I write inside of R
Studio and track with version control on GitHub.&lt;/p&gt;
&lt;p&gt;When I show to students that there are other options to Microsoft Word
and Powerpoint, their jaws drop, particularly when I show all that you
can do with a single plain-text document (integrating text, code,
commands, plots, citations and bibliography etc) that exports to pdf,
html, slides, and other output. Last semester, after I showed them
&lt;code&gt;R Markdown&lt;/code&gt;, several students told me it was the best thing about the
class, and some still use it for other assignments in other classes on
their own. This deserves its own series of posts, so for now I will
focus on the two or three things I tried to teach students &lt;em&gt;in addition&lt;/em&gt;
to how to use Markdown.&lt;/p&gt;
&lt;p&gt;First, &lt;strong&gt;R Projects&lt;/strong&gt; are absolutely essential. I discovered these late
in the game last year, but now realized that they solve far more
problems than trying to do without them.&lt;/p&gt;
&lt;p&gt;The #1 unnecessary problem I encounter with students is trying to load
data from external sources. The world is not full of tidy pre-cleaned
data, or even data that only come in &lt;code&gt;.csv&lt;/code&gt; formats. &lt;code&gt;tidyr&lt;/code&gt; is great
for that, and so is &lt;code&gt;readr&lt;/code&gt;, but the problem actually is one of basic
file operations on a computer: students (and R) don’t know where the
data is saved on their computers! Rather than trying to teach them how
to write out relative or absolute file paths to locate files on their
computer, R Projects solve this problem by setting the default working
directory to the folder where the project is stored on their computer.
That means that if you store the data file in that folder, you only need
to load it (with &lt;code&gt;readr&lt;/code&gt; or equivalent) with (e.g.)
&lt;code&gt;read_csv(&amp;quot;thedata.csv&amp;quot;&lt;/code&gt;), no more worrying about file paths!&lt;/p&gt;
&lt;p&gt;I also encourage students to create a logical folder hierarchy within
their projects, similar to what I show in &lt;a href=&#34;https://github.com/ryansafner/workflow&#34;&gt;this
repository&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Second, &lt;strong&gt;version control&lt;/strong&gt; with Github. I never actually got around to
showing this in class, but a number of students expressed interest in
learning how to do this. I feel it’s a bit advanced and requires a bit
more computing expertise (but not too much, since I’m able to pull it
off!), but I use it constantly. Perhaps more posts on this later.&lt;/p&gt;
&lt;p&gt;In any case, after teaching these workflow methods, looking back on the
first 3 R classes, I am tempted to just start from scratch with projects
and markdown and make students use them from the beginning. Perhaps next
year.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>New Course Websites</title>
      <link>https://ryansafner.com/post/new-course-websites/</link>
      <pubDate>Thu, 22 Aug 2019 17:28:00 +0000</pubDate>
      
      <guid>https://ryansafner.com/post/new-course-websites/</guid>
      <description>&lt;p&gt;This Fall semester, I have made dedicated websites for all of my courses at Hood College that host nearly all the course content. You can see them all &lt;a href=&#34;https://ryansafner.com/#teaching&#34;&gt;here&lt;/a&gt;. My interest was sparked when I saw &lt;a href=&#34;https://www.andrewheiss.com/teaching/&#34;&gt;Andrew Heiss&lt;/a&gt;&#39; amazing course websites.&lt;/p&gt;
&lt;p&gt;Until this point, all of my course content has lived on Blackboard for my students, though I have also tried to post syllabi and lecture slides (if not additional resources) on my &lt;a href=&#34;http://ryansafner.com/teaching&#34;&gt;personal website&lt;/a&gt; over the past few years. This process has been manageable, but has had way too much pointing-and-clicking, unnecessary duplication, and is not very open-source.&lt;/p&gt;
&lt;p&gt;For each course website, I am using the (surprisingly painless) combination of &lt;a href=&#34;https://rmarkdown.rstudio.com/&#34;&gt;R markdown&lt;/a&gt;, &lt;a href=&#34;http://github,com&#34;&gt;GitHub&lt;/a&gt;, &lt;a href=&#34;https://bookdown.org/yihui/blogdown/&#34;&gt;Blogdown&lt;/a&gt;, &lt;a href=&#34;https://gohugo.io/&#34;&gt;Hugo&lt;/a&gt;, and &lt;a href=&#34;http://netlify.com&#34;&gt;Netlify&lt;/a&gt; such that my entire workflow is reduced to:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Edit a plain text file with R Markdown (I like doing this in R Studio)&lt;/li&gt;
&lt;li&gt;Commit and push to GitHub (again, R Studio integrates this seamlessly)&lt;/li&gt;
&lt;li&gt;The new website is automatically updated by pushing my GitHub repository via Netlify&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Step three doesn&amp;rsquo;t even really count because once set up, I don&amp;rsquo;t have to do anything! I paid the fixed costs of extensively figuring out how everything works and experimenting over this past summer. If you are interested, I have attempted to explain the process of duplicating everything in this &lt;a href=&#34;https://github.com/ryansafner/classwebtest&#34;&gt;README file&lt;/a&gt; in my testing repository on GitHub (automatically rendered on the main page). It assumes some working knowledge of R Markdown, but not much else. One day I may make a better step-by-step guide if there is any interest, but lots of great guides on each feature already exist (and are linked to in the README).&lt;/p&gt;
&lt;p&gt;Each course has a page for the syllabus, assignments, reference (guides, links, and other resources to come), and a schedule page that is organized into relevant slides, assignments, or other resources for each class meeting. I have found the schedule page in particular to be a great framework for making all aspects of the course cohere in a single place.&lt;/p&gt;
&lt;p&gt;Why have I decided to do this?&lt;/p&gt;
&lt;p&gt;For good or ill, I am a slave to aesthetics^[I view lecture slides as a fine artform and have refused to use Microsoft Office products for over 8 years], and think that form matters just as much as function. This is not so much a rational viewpoint of mine as a vice: Producing and perfecting lectures is a &lt;em&gt;consumption good&lt;/em&gt; for me, not a means-to-an-end investment. I have delighted at learning &lt;a href=&#34;http://github.com/yihui/xaringan&#34;&gt;Xaringan&lt;/a&gt; this summer for all of my slides (guides and blog posts coming soon), and as they live in &lt;code&gt;html&lt;/code&gt;, hosting them on a single website made the most sense. I also made sure each course has its own custom hex sticker and workflow map (again, copying some killer design elements from Heiss). As a relevant side note, I have started to see the &lt;a href=&#34;https://www.authorea.com/users/5713/articles/19359-latex-is-dead-long-live-latex-typesetting-in-the-digital-age/_show_article&#34;&gt;limitations&lt;/a&gt; and &lt;a href=&#34;https://www.urban.org/urban-wire/your-data-deserve-better-pdf&#34;&gt;ugliness&lt;/a&gt; of &lt;a href=&#34;https://yihui.name/en/2013/10/markdown-or-latex/&#34;&gt;PDFs&lt;/a&gt; in the age of the web, and have ditched &lt;code&gt;LaTeX&lt;/code&gt; (admittedly, it was a long and loving relationship) for the elegance and beauty of  &lt;code&gt;markdown&lt;/code&gt; and &lt;code&gt;html&lt;/code&gt; wherever possible (posts on that to come).&lt;/p&gt;
&lt;p&gt;The main side effect is that all of my materials will be available to anyone online for free. I have always been committed to free and open source teaching, and have benefitted an extraordinary amount from learning from materials that other academics or practitioners - who I may never meet - have posted online.^[One of my guilty pleasures is to devour anyone&amp;rsquo;s lecture slides posted free online in areas that I teach or research and shamelessly steal what I like (I try to give credit if it is something major!). This confirms to me that I am a true academic.] While they will be without lecture videos (no plans for that, sorry), or original copies of some readings (copyright, my favorite), or answer keys for major assignments (I do recycle some questions), these websites are otherwise complete self-contained courses.&lt;/p&gt;
&lt;p&gt;Another reason this made sense for me is that as I have gotten into a rhythm of teaching the same course multiple times, I have started to accumulate a lot of additional resources: guides, tutorials, handouts on more technical concepts, etc. These would kill a lot of trees to print out for students, and it would be nice to have a central repository for them - a website is the natural place.&lt;/p&gt;
&lt;p&gt;There are some limitations, as I can&amp;rsquo;t do &lt;em&gt;everything&lt;/em&gt; on them. My major limitation is grading. Blackboard&amp;rsquo;s Grading Center is too convenient and centralized not to use (not to mention, secure, individualized, and FERPA-compliant). These new websites are primarily for broadcasting content to everyone, not using it as a personalized experience. Additionally, I will still use Blackboard for mass-emailing students. But my workflow of &lt;code&gt;1. write in plain text, 2. push to GitHub&lt;/code&gt; available for everyone seems vastly superior to endless mouse clicks and writing of individual items to a page viewable by a single course section (and then duplicating everything for a second section).&lt;/p&gt;
&lt;p&gt;It remains to be seen how my students will recieve these, but for now, I am excited.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Econometrics, Data Science, and Causal Inference</title>
      <link>https://ryansafner.com/post/econometrics-data-science-and-causal-inference/</link>
      <pubDate>Tue, 06 Aug 2019 10:57:00 +0000</pubDate>
      
      <guid>https://ryansafner.com/post/econometrics-data-science-and-causal-inference/</guid>
      <description>&lt;p&gt;This summer, I am overhauling my econometrics class in many ways, in part because I was pleased to recieve a teaching grant from my college to make more R resources for my econometrics class. &lt;a href=&#34;http://ryansafner.com/courses/ECON480&#34;&gt;Last Fall&lt;/a&gt; was the first time I had taught it using R, and I&amp;rsquo;ve learned a ton since then. Expect a flurry of posts in the coming weeks more on those topics.&lt;/p&gt;
&lt;p&gt;This post, however, explores some of the trends that I have been thinking about in teaching econometrics, and something monotonous that I have been struggling with that encapsulates the tension in these trends: what to name my course. I of course need to officially stick to the Procrustean Bed of the title in my college&amp;rsquo;s course catalog: ECON 480 - Econometrics, but in more causal conversation, or perhaps as a course subtitle, I have been torn between &lt;strong&gt;&amp;ldquo;data science&amp;rdquo;&lt;/strong&gt; and &lt;strong&gt;&amp;ldquo;causal inference.&amp;quot;&lt;/strong&gt; This class is an introduction to econometrics, and assumes students have taken basic statistics, so this &amp;ldquo;problem&amp;rdquo; is really just marketing on my part — it&amp;rsquo;s just a run-of-the-mill introductory econometrics course, but I like to add in a few bells and whistles!&lt;/p&gt;
&lt;p&gt;After thinking on this more, it seems to me there that we might be able to delineate a few possible directions that our field is heading. I hasten to qualify that I am thinking about this much more as a &lt;em&gt;teacher,&lt;/em&gt; of these topics, less as a &lt;em&gt;practitioner&lt;/em&gt;, as my own research niche is applied economic theory, albeit with a steadily-growing fascination with these methods. Nor am I a historian of econometric thought.&lt;/p&gt;
&lt;p&gt;These two trends or approaches I&amp;rsquo;ll call &amp;ldquo;data science&amp;rdquo; and &amp;ldquo;causal inference.&amp;rdquo; For the purposes of wrestling with this post, I will examine each of these forward-looking trends separately. I don&amp;rsquo;t want to rule out the possibility at all that these are entirely complementary, and I am making a big deal out of nothing. Nonetheless, I think it is an interesting distinction to examine.&lt;/p&gt;
&lt;h2 id=&#34;data-science&#34;&gt;&amp;ldquo;Data Science&amp;rdquo;&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.google.com/search?q=data+science&#34;&gt;&amp;ldquo;Data science&amp;rdquo;&lt;/a&gt; is the hip new catchall term for using statistical software to analyze data for a variety of purposes. It is commonly interpreted as &lt;a href=&#34;https://www.google.com/search?q=data+science+venn+diagram&#34;&gt;the intersection of statistics, computer science, and domain expertise (e.g. business, biology, etc)&lt;/a&gt;. I personally like the definition offered by the following tweet:&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Data Scientist (n.): Person who is better at statistics than any software engineer and better at software engineering than any statistician.&lt;/p&gt;&amp;mdash; Josh Wills (@josh_wills) &lt;a href=&#34;https://twitter.com/josh_wills/status/198093512149958656?ref_src=twsrc%5Etfw&#34;&gt;May 3, 2012&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;p&gt;The modal &amp;ldquo;data scientist&amp;rdquo; seems to be a young practitioner in industry or academia that use programming languages like R and Python to wrangle, visualize, and present data to solve empirical problems in their domain of expertise. On the academic/teaching side, self-styled &amp;ldquo;data scientists&amp;rdquo; seem to be trendy statisticians, computer scientists, or biologists (e.g. &amp;ldquo;bioinformatics&amp;rdquo;). &lt;a href=&#34;https://www.google.com/search?q=data+science+certificate&#34;&gt;New masters degrees and certifications&lt;/a&gt; in &amp;ldquo;Data Science&amp;rdquo; or &amp;ldquo;Data Analytics&amp;rdquo; seem to be popping up left and right at major universities.&lt;/p&gt;
&lt;p&gt;A large part of me wants to ride the coattails of this cool new trend and call/subtitle my class something like &amp;ldquo;Introduction to Data Science.&amp;rdquo; I decided against it for a few reasons that I think are instructive.&lt;/p&gt;
&lt;p&gt;First, I would be encroaching the turf of my colleagues in Statistics and Computer Science departments that often have courses with explicit titles like this. These classes are often a rehash of classic introductory statistics classes (probability, the normal distribution, $t$-tests, the central limit theorem, etc) but where software and simulation tend to replace pure theory and statistical proofs in a much more student-friendly way. It is a lot more intuitive and less tedious to understand a $p$-value via programming commands or simulating a null distribution of 10,000 bootstrapped samples than to learn the theoretical Student&amp;rsquo;s $t$-distributions and look up critical values from a table (as I once had to!).&lt;/p&gt;
&lt;p&gt;Second, although some econometrics textbooks do teach introductory econometrics this way^[&lt;a href=&#34;https://www.pearson.com/us/higher-education/product/Stock-Introduction-to-Econometrics-3rd-Edition/9780138009007.html&#34;&gt;&lt;em&gt;Stock and Watson&lt;/em&gt;&lt;/a&gt;, for example, which I taught with for 2 years], a good econometrics class, in my opinion, is much more than just an introductory statistics class (first review probability, distributions, statistical estimators, hypothesis testing, linear regression) that finally bleeds into the models that empirical econometricians &lt;em&gt;actually&lt;/em&gt; — oh no, the semester&amp;rsquo;s already over!&lt;/p&gt;
&lt;p&gt;Econometrics has a particularly opinionated view of statistical models, and often uses them to a very different end than most &amp;ldquo;data science&amp;rdquo;-type uses. This is precisely what distinguishes uses of statistics in &lt;em&gt;economics&lt;/em&gt; from its use in other domain areas^[Epidemiology is perhaps closest to economics in this regard.], the focus on the second major trend I discuss below, &lt;strong&gt;causal inference.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Perhaps the paradigmatic application of &amp;ldquo;data science&amp;rdquo; beyond mere descriptive statistics and causal inference is &amp;ldquo;machine learning&amp;rdquo; and associated terms and techniques (e.g. &amp;ldquo;big data,&amp;rdquo; &amp;ldquo;neural networks,&amp;rdquo; &amp;ldquo;artificial intelligence,&amp;rdquo; etc). These issues often deal with massive &amp;ldquo;wide&amp;rdquo; datasets, frequently with more variables than observations. Take a website such as Facebook, which has a ton of information about a user &amp;ndash; locations they visit the website from, who their friends are, what pages they have interacted with, etc. &lt;a href=&#34;https://www.youtube.com/watch?v=R9OHn5ZF4Uo&#34;&gt;Machine learning techniques&lt;/a&gt; such as neural networks, as I understand them, try to identify what characteristics predict a certain outcome. Facial recognition software seems to me to operate similarly: take a large number of pre-identified images (e.g. a human at some point determined the image indeed contained a face) for &lt;em&gt;training&lt;/em&gt; an algorithm, pass those images through $n$-number of layers to detect common characteristics of &amp;ldquo;faces&amp;rdquo; vs. &amp;ldquo;not-faces&amp;rdquo;, and then additional subtleties that differentiate one face from another, and after $n$-layers and $m$ features are determined (where both $n$ and $m$ are massive numbers!), then the algorithm can take a &lt;em&gt;new&lt;/em&gt; un-classified image and determine &lt;em&gt;if&lt;/em&gt; it includes a face, or, if it is a very good algorithm, &lt;em&gt;whose&lt;/em&gt; face it is. But, as &lt;a href=&#34;https://www.youtube.com/watch?v=R9OHn5ZF4Uo&#34;&gt;this excellent video &lt;/a&gt; reiterates, &lt;em&gt;nobody understands&lt;/em&gt; how the neural network that successfully identifies faces &lt;em&gt;works&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;The key feature here is that it these technqiues are atheoretical, they are entirely empirical and judged on their rate of success. The algorithm itself has no underlying theory behind what &amp;ldquo;causes&amp;rdquo; a face or determines a face, only a purely &lt;em&gt;empirical&lt;/em&gt; process of identifying patterns statistically by brute force. Nobody actually knows, or perhaps even could know, &lt;em&gt;why&lt;/em&gt; or &lt;em&gt;how&lt;/em&gt; an algorithm works.&lt;/p&gt;
&lt;h2 id=&#34;causal-inference&#34;&gt;&amp;ldquo;Causal Inference&amp;rdquo;&lt;/h2&gt;
&lt;p&gt;Causal inference, by contrast, whatever some hardcore practitioners might say about merely &amp;ldquo;letting the data speak for itself&amp;rdquo;, &lt;em&gt;necessitates&lt;/em&gt; an underlying theory to explain a relationship or a process.&lt;/p&gt;
&lt;p&gt;Without going into a full post on methodology, economics is a theory-driven discipline in a way like few others. This is why I expect that &amp;ldquo;big data&amp;rdquo; and &amp;ldquo;machine learning&amp;rdquo; techniques will offer only marginal insights in economic research relative to the tried-and-true methods (and I am &lt;a href=&#34;https://mru.org/courses/mastering-econometrics/are-machine-learning-and-big-data-changing-econometrics&#34;&gt;in good company&lt;/a&gt;). It is certainly true that some theories may be verified or disproven by data, or entirely new theories emerge via analyzing data that may never have otherwise been discovered. However, the main tool of empirical work is to identify and measure causal effects &lt;em&gt;already&lt;/em&gt; theorized or hypothesized from solid economic theory.^[I don&amp;rsquo;t want to take the hardcore position here that there is &lt;em&gt;no&lt;/em&gt; validity to empirical work in economics. Sometimes multiple economic principles might be operating in combination or against each other through multiple channels in the real world - empirical work helps us to tease them out in a useful way.] Data can never &amp;ldquo;speak for itself,&amp;rdquo; because we wouldn&amp;rsquo;t know &lt;em&gt;what data&lt;/em&gt; is worth collecting in the first place. I like to think about it with the (&lt;a href=&#34;https://en.wikiquote.org/wiki/Immanuel_Kant#section_24&#34;&gt;attributed&lt;/a&gt;) Immanuel Kant quote:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Experience without theory is blind, while theory without experience is mere intellectual play.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Economists and econometricians have always emphasized the role of causal inference in econometrics. I recall learning - to my surprise - as an undergraduate and as a graduate in my courses that with clever identification strategies (instrumental variables, regression discontinuity, differences-in-differences, fixed effects, etc), we &lt;em&gt;can&lt;/em&gt; do a lot more than just say things are &amp;ldquo;correlated,&amp;rdquo; and thus gore the sacred refrain of statistics professors everywhere (&lt;em&gt;&amp;ldquo;correlation does not imply causation&amp;rdquo;&lt;/em&gt;). I never experienced what learning econometrics was like in the 1980s or 1990s, but this view has only grown stronger with the &amp;ldquo;credibility revolution&amp;rdquo; described by Angrist and Pischke (2010, 4):&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Empirical microeconomics has experienced a credibility revolution, with a consequent increase in policy relevance and scientific impact…[A]s we see it, the primary engine driving improvement has been a focus on  the quality of empirical research designs.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Having much more omnipresent and higher-quality micro-level data has also helped.&lt;/p&gt;
&lt;p&gt;However, I have noticed in recent years in a particular strand of econometrics teaching has gotten more pathological about causal inference (I don&amp;rsquo;t mean to use that term pejoratively!). Not all of the work in this trend is by economists, but it seems to finally be seeping into econometrics pedagogy.&lt;/p&gt;
&lt;p&gt;I think the quintessential tools for this new wave of teaching causal inference are the &lt;a href=&#34;https://en.wikipedia.org/wiki/Directed_acyclic_graph&#34;&gt;Directed Acyclic Graphs (DAGs)&lt;/a&gt; popularized by famed computer scientist Judea Pearl, or the &lt;a href=&#34;https://www.google.com/search?client=q=do+calculus&#34;&gt;&amp;ldquo;do-calculus&amp;rdquo;&lt;/a&gt; for more advanced treatments. I see this now showing up in excellent teaching materials like Scott Cunningham&amp;rsquo;s &lt;a href=&#34;https://scunning.com/cunningham_mixtape.pdf&#34;&gt;&lt;em&gt;Causal Inference: the Mixtape&lt;/em&gt;&lt;/a&gt;, &lt;a href=&#34;http://www.michaelnielsen.org/ddi/if-correlation-doesnt-imply-causation-then-what-does/&#34;&gt;this great but technical blog post&lt;/a&gt; by Michael Nielson, a lecture in Andrew Heiss&#39; &lt;a href=&#34;https://econw19.classes.andrewheiss.com/class/27-class/&#34;&gt;class&lt;/a&gt;, and Nick Huntington-Klein&amp;rsquo;s excellent &lt;a href=&#34;http://www.nickchk.com/econ305.html&#34;&gt;lecture slides&lt;/a&gt; for his course in Economics, Causality, and Analytics (now there&amp;rsquo;s a good title!). More and more people also seem to be linking to the great &lt;a href=&#34;http://dagitty.net&#34;&gt;daggity.net&lt;/a&gt; tool for drawing and interpreting DAGs.&lt;/p&gt;
&lt;p&gt;After reading Pearl and Mackenzie (2018)&amp;rsquo;s &lt;em&gt;The Book of Why&lt;/em&gt;, I briefly felt much more zealous about claiming that we can indeed argue &amp;ldquo;&lt;em&gt;X does cause Y, dammit!&amp;quot;&lt;/em&gt;. The review of DAGs by &lt;a href=&#34;https://arxiv.org/abs/1907.07271&#34;&gt;Imbens (2019)&lt;/a&gt; brought me back down to earth. DAGs are a great pedagogical tool^[There are limitations to them, such as the inability to add cutoff variables for regression discontinuity design models, or interaction terms. Huntington-Klein seems to fudge this by creating nodes like $X&amp;lt;c$, or $X * Z$, for the respective problems mentioned)] but aren&amp;rsquo;t the silver bullet for clarifying empirical work. Imbens makes a compelling case that DAGs have very little practicality beyond &amp;ldquo;toy models&amp;rdquo; such as those in Pearl and Mackenzie (2018) and standard econometric research designs since the &amp;ldquo;credibility revolution&amp;rdquo; (Angrist and Pischke, 2010) work far better in practice. For the moment, that&amp;rsquo;s fine for me — I may make a post later about the benefits^[Namely, 1. making model assumptions explicit, 2. using proper correlations in the data to falsify a proposed causal model, and 3. showing that there are not all variables need to be controlled for, indeed controlling for certain types of variables actually &lt;em&gt;introduces&lt;/em&gt; bias!] — I think my students will find them very useful. I am more convinced with Pearl&amp;rsquo;s argument that causal models are a necessary ingredient to the progress of artificial intelligence.&lt;/p&gt;
&lt;h2 id=&#34;a-compromise&#34;&gt;A Compromise&lt;/h2&gt;
&lt;p&gt;In any case, thinking about these two strands makes the development of the field seem richer and more dynamic. I again add that I am not trying to artificially carve out an antagonism between these two approaches, in fact I hope to marry them into the latest iteration of my econometrics course. One of the appeals I try to make to my students (some of whom are Economics majors, some Business majors, and a mixture of others such as Biology or Environmental Science) is that the tools we learn in this class will not only make you better consumers of science and studies, but also may be invaluable in the emerging job market. &amp;ldquo;Data science&amp;rdquo; again is one of the sexy trends of the 2010s and beyond. Learning how to import, wrangle, analyze, and present data with statistical software and programming languages like R already makes students budding &amp;ldquo;data scientists.&amp;rdquo; Futhermore, &lt;em&gt;causal inference&lt;/em&gt; with empirical models and research designs is the &lt;em&gt;economist&amp;rsquo;s&lt;/em&gt; comparative advantage that will set &lt;em&gt;economics&lt;/em&gt; majors apart from your average &amp;ldquo;data science&amp;rdquo; or &amp;ldquo;data analytics&amp;rdquo; certificate-holder.&lt;/p&gt;
&lt;h1 id=&#34;references&#34;&gt;References&lt;/h1&gt;
&lt;p&gt;Angrist, Joshua D, and Jorn-Steffen Pischke, 2010, &amp;ldquo;The Credibility Revolution in Empirical Economics: How Better Research Design is Taking the Con out of Econometrics,&amp;rdquo; &lt;em&gt;Journal of Economic Perspectives&lt;/em&gt; &lt;em&gt;24&lt;/em&gt;(2), 3–30.&lt;/p&gt;
&lt;p&gt;Imbens, Guido W, 2019, &amp;ldquo;Potential Outcome and Directed Acyclic Graph Approaches to Causality: Relevance for Empirical Practice in Economics,&amp;rdquo; Manuscript&lt;/p&gt;
&lt;p&gt;Pearl, Judea and Dana Mackenzie, 2018, &lt;em&gt;The Book of Why&lt;/em&gt;, Allen Lane&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Replicating Stata&#39;s &#39;Robust&#39; Option for OLS Standard Errors in R</title>
      <link>https://ryansafner.com/post/replicating-statas-robust-option-for-ols-standard-errors-in-r/</link>
      <pubDate>Fri, 28 Dec 2018 20:11:00 +0000</pubDate>
      
      <guid>https://ryansafner.com/post/replicating-statas-robust-option-for-ols-standard-errors-in-r/</guid>
      <description>&lt;p&gt;One of the advantages of using Stata for linear regression is that it can automatically use heteroskedasticity-robust standard errors simply by adding &lt;code&gt;, r&lt;/code&gt; to the end of any regression command. Anyone can more or less use robust standard errors and make more accurate inferences without even thinking about what they represent or how they are determined since it&amp;rsquo;s so easy just to add the letter &lt;code&gt;r&lt;/code&gt; to any regression.&lt;/p&gt;
&lt;p&gt;In &lt;code&gt;R&lt;/code&gt;, robust standard errors are not &amp;ldquo;built in&amp;rdquo; to the base language. There are a few ways that I&amp;rsquo;ve discovered to try to replicate Stata&amp;rsquo;s &amp;ldquo;robust&amp;rdquo; command. None of them, unfortunately, are as simple as typing the letter &lt;code&gt;r&lt;/code&gt; after a regression. Each has its ups and downs, but may serve different purposes.&lt;/p&gt;
&lt;p&gt;Below, I will demonstrate the two methods, but first, let&amp;rsquo;s create some random data that will have heteroskedastic residuals.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-R&#34; data-lang=&#34;R&#34;&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;set.seed&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;20&lt;/span&gt;)
&lt;span style=&#34;color:#75715e&#34;&gt;# draw 500 observations from a random uniform distribution (runif) between 0 and 10.  &lt;/span&gt;
x&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;runif&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;500&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;)

&lt;span style=&#34;color:#75715e&#34;&gt;# draw 500 observations from a random normal distribution&lt;/span&gt;
y&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;rnorm&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;500&lt;/span&gt;,mean&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;x,sd&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;x) &lt;span style=&#34;color:#75715e&#34;&gt;# set mean and sd are set as the value of each x value&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# thus, as x gets larger, so does the sd, and thus the residuals&lt;/span&gt;

data&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;data.frame&lt;/span&gt;(x,y) &lt;span style=&#34;color:#75715e&#34;&gt;# make x and y variables in a dataframe called &amp;#34;data&amp;#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We then run a regression of &lt;code&gt;y&lt;/code&gt; on &lt;code&gt;x&lt;/code&gt;:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-{r}&#34; data-lang=&#34;{r}&#34;&gt;reg&amp;lt;-lm(y~x)
summary(reg)
&lt;/code&gt;&lt;/pre&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;## 
## Call:
## lm(formula = y ~ x)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -24.7115  -2.4865  -0.3479   2.9699  20.6123 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)  0.68508    0.52264   1.311    0.191    
## x            0.76484    0.08887   8.607   &amp;lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 5.949 on 498 degrees of freedom
## Multiple R-squared:  0.1295,	Adjusted R-squared:  0.1277 
## F-statistic: 74.08 on 1 and 498 DF,  p-value: &amp;lt; 2.2e-16
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Let&amp;rsquo;s plot a scatterplot to visualize the data and add the regression line. Clearly, the data is &amp;ldquo;fan&amp;rdquo; shaped, centered on the regression line, but with larger and larger residuals (distance between the regression line and the data point, $\hat{\epsilon}_i=\hat{Y}_i-Y_i$) as $X$ gets larger.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;plot-1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;I have also broken up the scatterplot into 5 different sections over the range of &lt;code&gt;x&lt;/code&gt; values. Below, I plot density plots of the residuals over each of the 5 different ranges of &lt;code&gt;x&lt;/code&gt; values, and we can clearly see that the variance of the residuals dramatically increases as &lt;code&gt;x&lt;/code&gt; increases.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;residualsplots.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Using the &lt;code&gt;lmtest&lt;/code&gt; package, we can also formally run a Breusch-Pagan test for heteroskedasticity.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;library&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;lmtest&amp;#34;&lt;/span&gt;)
&lt;span style=&#34;color:#a6e22e&#34;&gt;bptest&lt;/span&gt;(reg)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;## 
## 	studentized Breusch-Pagan test
## 
## data:  reg
## BP = 90.547, df = 1, p-value &amp;lt; 2.2e-16
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;method-1-sandwich-package&#34;&gt;Method 1: &lt;code&gt;Sandwich&lt;/code&gt; package&lt;/h2&gt;
&lt;p&gt;In order to understand what the &amp;ldquo;fix&amp;rdquo; in this method is actually doing, we also need to look &amp;ldquo;under the hood&amp;rdquo; of what &lt;code&gt;R&lt;/code&gt; is doing when it runs OLS and stores everything in the &lt;code&gt;lm&lt;/code&gt; regression object.&lt;/p&gt;
&lt;p&gt;One thing stored in &lt;code&gt;reg&lt;/code&gt; is the variance-covariance matrix, estimating the covariance of each OLS estimator (the &amp;ldquo;betas&amp;rdquo;) with every other OLS estimator:&lt;/p&gt;
&lt;p&gt;$$ \begin{pmatrix} cov(\hat{\beta_0},\hat{\beta_0}) &amp;amp; cov(\hat{\beta_0},\hat{\beta_1}) &amp;amp; \cdots &amp;amp; cov(\hat{\beta_0},\hat{\beta_k})\\\ cov(\hat{\beta_1},\hat{\beta_0}) &amp;amp; cov(\hat{\beta_1},\hat{\beta_1}) &amp;amp; \cdots &amp;amp; cov(\hat{\beta_1},\hat{\beta_k})\\\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\\ cov(\hat{\beta_k},\hat{\beta_0}) &amp;amp; cov(\hat{\beta_k},\hat{\beta_1}) &amp;amp; \cdots &amp;amp; cov(\hat{\beta_k},\hat{\beta_k})\\\ \end{pmatrix}$$&lt;/p&gt;
&lt;p&gt;Since the covariance of anything with itself is the variance, the &lt;em&gt;diagonal&lt;/em&gt; elements of this matrix are the variances of the OLS estimators:&lt;/p&gt;
&lt;p&gt;$$\begin{pmatrix}var(\hat{\beta_0}) &amp;amp; cov(\hat{\beta_0},\hat{\beta_1}) &amp;amp; \cdots &amp;amp; cov(\hat{\beta_0},\hat{\beta_k})\\\ cov(\hat{\beta_1}, \hat{\beta_0}) &amp;amp; var(\hat{\beta_1}) &amp;amp; \cdots &amp;amp; cov(\hat{\beta_1},\hat{\beta_k})\\\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\\ cov(\hat{\beta_k},\hat{\beta_0}) &amp;amp; cov(\hat{\beta_k},\hat{\beta_1}) &amp;amp; \cdots &amp;amp; var(\hat{\beta_k})\\\ \end{pmatrix}$$&lt;/p&gt;
&lt;p&gt;So if we look at the simple $2 \times 2$ variance-covariance matrix in our simple &lt;code&gt;reg&lt;/code&gt; using &lt;code&gt;vcov&lt;/code&gt;, we see.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;vcov&lt;/span&gt;(reg)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;##             (Intercept)            x
## (Intercept)  0.27315741 -0.039976551
## x           -0.03997655  0.007897029
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;We can extract just the diagonal of the matrix with &lt;code&gt;diag()&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;diag&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;vcov&lt;/span&gt;(reg))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;## (Intercept)           x 
## 0.273157410 0.007897029
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;These are the variances of $\hat{\beta_0}$ and $\hat{\beta_1}$. Since the standard error of an estimator is the square root of its variance, we simply square root these values to get the standard errors of $\hat{\beta_0}$ and $\hat{\beta_1}$, which were originally reported in our regression output next to the coefficient estimates.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;sqrt&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;diag&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;vcov&lt;/span&gt;(reg)))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;## (Intercept)           x 
##  0.52264463  0.08886523
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Now, the whole problem is we know that due to heteroskedasticity, the standard errors are incorrectly estimated. To fix this, we use the &lt;code&gt;sandwich&lt;/code&gt; package that allows us to manually recalculate the variance-covariance matrix using methods robust to heteroskedasticity. This is why I went through the trouble of describing the variance-covariance matrix above, as we will be recalculating it using a different method, the &lt;code&gt;HC1&lt;/code&gt; method, which is how Stata calculates it. I then store these calculates as &lt;code&gt;rse&lt;/code&gt; in my original &lt;code&gt;lm&lt;/code&gt; object called &lt;code&gt;reg&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;library&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;sandwich&amp;#34;&lt;/span&gt;) &lt;span style=&#34;color:#75715e&#34;&gt;# package that allows for robust SE estimation&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# create Robust Standard Errors for regression as &amp;#39;reg$rse&amp;#39;&lt;/span&gt;
reg&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;rse &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;sqrt&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;diag&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;vcovHC&lt;/span&gt;(reg, type&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;HC1&amp;#34;&lt;/span&gt;)))
&lt;span style=&#34;color:#75715e&#34;&gt;# same procedure as above but now we generate vcov with &amp;#34;HC1&amp;#34; method&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;If we now want to recreate the regression output table produced by &lt;code&gt;summary(reg)&lt;/code&gt;, we need to use the &lt;code&gt;coeftest&lt;/code&gt; function, which is a part of the &lt;code&gt;lmtest&lt;/code&gt; package. Just to verify, if we run &lt;code&gt;coeftest()&lt;/code&gt; on our original &lt;code&gt;reg&lt;/code&gt;, it prints the regression output table with coefficient estimates, standard errors, $t$-statistics, and $p$-values.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;coeftest&lt;/span&gt;(reg) &lt;span style=&#34;color:#75715e&#34;&gt;# test with normal SEs&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;## 
## t test of coefficients:
## 
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept) 0.685077   0.522645  1.3108   0.1905    
## x           0.764836   0.088865  8.6067   &amp;lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;If we run it again, but set the &lt;code&gt;vcov&lt;/code&gt; option to &lt;code&gt;ccovHC(reg, &amp;quot;HC1&amp;quot;)&lt;/code&gt;, it will print the robust standard errors.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;coeftest&lt;/span&gt;(reg,vcov&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;vcovHC&lt;/span&gt;(reg,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;HC1&amp;#34;&lt;/span&gt;)) &lt;span style=&#34;color:#75715e&#34;&gt;# tests with robust SEs&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;## 
## t test of coefficients:
## 
##             Estimate Std. Error t value  Pr(&amp;gt;|t|)    
## (Intercept) 0.685077   0.312060  2.1953    0.0286 *  
## x           0.764836   0.093579  8.1732 2.504e-15 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;These command simply print the robust standard errors for us to see in the console as we run our analyses. If we want to take these and actually output them in a presentable regression table, we will use the well-known &lt;code&gt;stargazer&lt;/code&gt; package, used to take &lt;code&gt;R&lt;/code&gt; regression &lt;code&gt;lm&lt;/code&gt; objects and print scholarly journal-quality regression tables.&lt;/p&gt;
&lt;p&gt;The nice thing is &lt;code&gt;stargazer&lt;/code&gt; has an option to set where the standard errors are pulled from. We stored our robust standard errors in &lt;code&gt;reg&lt;/code&gt; as a vector called &lt;code&gt;rse&lt;/code&gt;. Below, I print the &lt;code&gt;stargazer&lt;/code&gt; regression table (with several personalized options) for this webpage, showing the our regression twice, once with the normal standard errors, and the second time with the robust standard errors. For more, see &lt;a href=&#34;https://www.jakeruss.com/cheatsheets/stargazer/#robust-standard-errors-replicating-statas-robust-option&#34;&gt;Jake Russ&#39; cheat sheet&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;library&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;stargazer&amp;#34;&lt;/span&gt;)
&lt;span style=&#34;color:#a6e22e&#34;&gt;stargazer&lt;/span&gt;(reg, reg, 
          se&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;list&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;NULL&lt;/span&gt;,reg&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;rse), 
          type&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;html&amp;#34;&lt;/span&gt;,
          column.labels &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;c&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Normal&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Robust SEs&amp;#34;&lt;/span&gt;), 
          title&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Regression Results&amp;#34;&lt;/span&gt;, 
          dep.var.caption &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;,
          omit.stat&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;c&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;adj.rsq&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;f&amp;#34;&lt;/span&gt;)) 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The key to notice is &lt;code&gt;se=list(NULL,reg$rse)&lt;/code&gt;, which creates a list of objects from which to pull the standard errors for each regression in the table. The first regression uses the standard methods, needing no special source, so it is set to &lt;code&gt;NULL&lt;/code&gt;. The second regression, also &lt;code&gt;reg&lt;/code&gt;, uses our robust standard errors stored in &lt;code&gt;reg$rse&lt;/code&gt;. The output of the table is below:&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;A casual search around the internet, as well as the textbook that I use shows that this is the most common or reccomended method for achieving robust standard errors.&lt;/p&gt;
&lt;h2 id=&#34;method-2-using-estimatr&#34;&gt;Method 2: Using &lt;code&gt;estimatr&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;I recently discovered another package called &lt;a href=&#34;https://github.com/DeclareDesign/estimatr&#34;&gt;&lt;code&gt;estimatr&lt;/code&gt;&lt;/a&gt; that achieves the simplicity of changing a single word, just like in Stata.&lt;/p&gt;
&lt;p&gt;Loading the &lt;code&gt;estimatr&lt;/code&gt; package, all we need to do is create a new regression (I&amp;rsquo;ll call it &lt;code&gt;reg.robust&lt;/code&gt;) and instead of running a normal linear model with &lt;code&gt;lm&lt;/code&gt;, we run &lt;code&gt;lm_robust&lt;/code&gt;, and set the standard errors &lt;code&gt;se_type=&amp;quot;stata&amp;quot;&lt;/code&gt; to calculate using the HC1 method (same as above).&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;library&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;estimatr&amp;#34;&lt;/span&gt;)
reg.robust&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;lm_robust&lt;/span&gt;(y&lt;span style=&#34;color:#f92672&#34;&gt;~&lt;/span&gt;x,se_type &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;stata&amp;#34;&lt;/span&gt;)
&lt;span style=&#34;color:#a6e22e&#34;&gt;summary&lt;/span&gt;(reg.robust)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;## 
## Call:
## lm_robust(formula = y ~ x, se_type = &amp;quot;stata&amp;quot;)
## 
## Standard error type:  HC1 
## 
## Coefficients:
##             Estimate Std. Error t value  Pr(&amp;gt;|t|) CI Lower CI Upper  DF
## (Intercept)   0.6851    0.31206   2.195 2.860e-02  0.07196   1.2982 498
## x             0.7648    0.09358   8.173 2.504e-15  0.58098   0.9487 498
## 
## Multiple R-squared:  0.1295 ,	Adjusted R-squared:  0.1277 
## F-statistic:  66.8 on 1 and 498 DF,  p-value: 2.504e-15
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;We can see the standard errors are now identical to the robust ones from the method above.&lt;/p&gt;
&lt;p&gt;One other nicety of &lt;code&gt;estimatr&lt;/code&gt; is that &lt;a href=&#34;https://declaredesign.org/r/estimatr/articles/estimatr-in-the-tidyverse.html&#34;&gt;it can create &lt;code&gt;tidy data.frame&lt;/code&gt; versions&lt;/a&gt; of &lt;code&gt;R&lt;/code&gt; default regression output tables, much like the &lt;code&gt;broom&lt;/code&gt; package in the &lt;code&gt;tidyverse&lt;/code&gt;. We do this simply with the &lt;code&gt;tidy()&lt;/code&gt; command on our &lt;code&gt;reg.robust&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;tidy&lt;/span&gt;(reg.robust)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;Until today, I thought that was all &lt;code&gt;estimatr&lt;/code&gt; could do: it could show us the robust standard errors, but we could not present it in an output table with &lt;code&gt;stargazer&lt;/code&gt;. &lt;code&gt;lm_robust&lt;/code&gt; objects do not get along well with &lt;code&gt;stargazer&lt;/code&gt;, only &lt;code&gt;lm&lt;/code&gt; objects.&lt;/p&gt;
&lt;p&gt;Documentation is extremely scarce, but there is a &lt;code&gt;starprep()&lt;/code&gt; command to enable use of &lt;code&gt;estimatr&lt;/code&gt; &lt;code&gt;lm_robust&lt;/code&gt; objects with &lt;code&gt;stargazer&lt;/code&gt;. After a lot of searching and trial and error, the process seems to be that using &lt;code&gt;starprep&lt;/code&gt; extracts &lt;em&gt;only&lt;/em&gt; the (robust) standard errors from the &lt;code&gt;lm_robust&lt;/code&gt; regression, meaning we just need to insert this into &lt;code&gt;stargazer&lt;/code&gt;&amp;rsquo;s &lt;code&gt;se=&lt;/code&gt; option.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# this is what starprep extracts&lt;/span&gt;
&lt;span style=&#34;color:#a6e22e&#34;&gt;starprep&lt;/span&gt;(reg.robust)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;## [[1]]
## (Intercept)           x 
##  0.31205969  0.09357893
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Below, again, I run &lt;code&gt;stargazer&lt;/code&gt; on our original &lt;code&gt;reg&lt;/code&gt; twice, with the second instance using robust standard errors via &lt;code&gt;estimatr&lt;/code&gt;. Specifically notice the list for &lt;code&gt;se&lt;/code&gt;; again, like the fist method above, we use the default for the first regression (hence &lt;code&gt;NULL&lt;/code&gt;), and for the second, we use &lt;code&gt;starprep(reg.robust)&lt;/code&gt; to extract from &lt;code&gt;estimatr&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;stargazer&lt;/span&gt;(reg, reg, 
          se&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;starprep&lt;/span&gt;(reg,reg.robust), 
          type&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;html&amp;#34;&lt;/span&gt;,
          column.labels &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;c&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Normal&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Robust SEs&amp;#34;&lt;/span&gt;), 
          title&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Regression Results&amp;#34;&lt;/span&gt;, 
          dep.var.caption &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;,
          omit.stat&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;c&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;adj.rsq&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;f&amp;#34;&lt;/span&gt;)) 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;Again, we can see both methods achieve results identical to Stata. The nice thing about &lt;code&gt;estimatr&lt;/code&gt; is we do not need to mess around with the variance-covariance matrix!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Visualizing Tax Incidence with Shiny</title>
      <link>https://ryansafner.com/post/visualizing-tax-incidence-with-shiny/</link>
      <pubDate>Sun, 07 Oct 2018 18:35:00 +0000</pubDate>
      
      <guid>https://ryansafner.com/post/visualizing-tax-incidence-with-shiny/</guid>
      <description>&lt;p&gt;As I have mentioned in &lt;a href=&#34;https://ryansafner.com/post/visualizing-linear-regression-with-shiny/&#34;&gt;other posts&lt;/a&gt;, this semester I trying to maximize the number of intuitive visualizations for my students to master economic and mathematical concepts more in my teaching, as well as develop my own &lt;code&gt;R&lt;/code&gt; skills. For my &lt;a href=&#34;http://ryansafner.com/courses/econ306&#34;&gt;intermediate microeconomics course&lt;/a&gt;, I recently put together an  &lt;a href=&#34;https://ryansafner.shinyapps.io/tax-incidence/&#34;&gt;interactive Shiny App&lt;/a&gt; with &lt;code&gt;R&lt;/code&gt; that demonstrates the effect of a tax on consumers and producers.&lt;/p&gt;
&lt;p&gt;Simply input your own (inverse) demand and supply functions and the size of the tax, and the graph and summary will update with the market equilibrium and the surpluses lost by the tax. This will allow students to see several effects that we commonly teach in microeconomics:&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;The economic burden of the tax depends on the relative price elasticities of demand vs. supply.&lt;/strong&gt; Whomever is mandated to pay by statute may not end up being the party that bears the entire tax. Whomever has a greater elasticity (demand or supply) will bear &lt;em&gt;less&lt;/em&gt; of the tax, and vice versa. I teach my students to think of &amp;ldquo;elasticity&amp;rdquo; as being equivalent to one&amp;rsquo;s ability to &amp;ldquo;escape&amp;rdquo; a tax. The Shiny apps shows that simply &lt;em&gt;increasing&lt;/em&gt; the slope parameter of Supply or Demand will &lt;em&gt;lower&lt;/em&gt; its&#39; elasticity, and &lt;em&gt;raise&lt;/em&gt; the share of the tax borne by that side of the market.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Larger taxes lead to larger market distortions.&lt;/strong&gt; Tax revenue increases at a rate less than $t$ and deadweight loss increases at a rate greater than $t$. Simply increase the tax rate and see how government revenue changes and deadweight loss changes. This is why economists call for &amp;ldquo;low rates and broad bases.&amp;rdquo;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The visualization can also show the effects of extremes: perfectly elastic demand or supply (simply set $slope=0$). &lt;strong&gt;A perfectly elastic curve bears none of the tax and shifts it entirely to the other side of the amrket.&lt;/strong&gt; The calculated burden for that party will be 0, and the other party&amp;rsquo;s share will unfortunately mess up to read $&amp;ldquo;NaN%&amp;quot;$,&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; but this is to be interpretted as $100%$. The visualization cannot show the effect of perfectly &lt;em&gt;inelastic&lt;/em&gt; demand or supply (vertical lines) since I cannot set a slope slider value to be $\infty$.&lt;/p&gt;
&lt;p&gt;To give credit where it is due, I was originally inspired by &lt;a href=&#34;http://home.uchicago.edu/cbm4/econ260/E260tidemo.html&#34;&gt;this visualization&lt;/a&gt; apparently made in Mathematica for Casey Mulligan&amp;rsquo;s &lt;a href=&#34;http://home.uchicago.edu/cbm4/econ260/&#34;&gt;Public Finance class&lt;/a&gt; at the University of Chicago.&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;For more on the economics, see Lecture 10 slides on this topic from the course page.&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;This is since I calculated each share according to the common formula: $\frac{|\text{elasticity of other party}|}{(|E_D|+E_S)}$. If instead I had calculated the change in Consumer Surplus and change in Producer Surplus, we could get accurate numbers.&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>Visualizing Linear Regression with Shiny</title>
      <link>https://ryansafner.com/post/visualizing-linear-regression-with-shiny/</link>
      <pubDate>Fri, 28 Sep 2018 15:55:00 +0000</pubDate>
      
      <guid>https://ryansafner.com/post/visualizing-linear-regression-with-shiny/</guid>
      <description>&lt;p&gt;For my &lt;a href=&#34;http://ryansafner.com/courses/econ480&#34;&gt;econometrics course&lt;/a&gt; this semester, I have been using &lt;code&gt;R&lt;/code&gt; to help students visualize linear regression models. Running a regression in &lt;code&gt;R&lt;/code&gt; is quite simple, as is intepretting the results, with a little bit of training. However, I emphasize that I want students to &lt;em&gt;understand&lt;/em&gt; what is happening &amp;ldquo;inside the black box&amp;rdquo; of regression. I discourage blindly trusting  &lt;code&gt;R&lt;/code&gt;&amp;rsquo;s opaquely simple input and output, and get students to learn what &lt;code&gt;R&lt;/code&gt; is doing &lt;em&gt;under the hood&lt;/em&gt;, even if they will never have to manually estimate the model themselves.&lt;/p&gt;
&lt;h3 id=&#34;brief-econometrics-review-incoming&#34;&gt;Brief Econometrics Review Incoming:&lt;/h3&gt;
&lt;p&gt;Ordinary Least Squares (OLS) regression simply chooses the intercept ($\hat{\beta_0}$) and slope ($\hat{\beta_1}$) parameters for the equation of a line that best fits the data:
$$\hat{Y_i}=\hat{\beta_0}+\hat{\beta_1}X_i$$&lt;/p&gt;
&lt;p&gt;by trying to minimize the &lt;strong&gt;Sum of Squared Errors (SSE)&lt;/strong&gt;. The error $\epsilon$ (or residual) of an observation is defined as the difference between the &lt;em&gt;actual&lt;/em&gt; value of $Y$ observed in the data associated with a given value of $X$, and the &lt;em&gt;predicted&lt;/em&gt; value, $\hat{Y}$ given $X$:&lt;/p&gt;
&lt;p&gt;$$\hat{\epsilon}=Y_i-\hat{Y_i}$$&lt;/p&gt;
&lt;p&gt;So what OLS does is minimize the sum of the squared errors:&lt;/p&gt;
&lt;p&gt;$$\min \sum^n_{i=1} \hat{\epsilon_i}^2$$&lt;/p&gt;
&lt;p&gt;This is a calculus problem that is solvable, if tedious. But at least it should be intuitive to understand what OLS does.&lt;/p&gt;
&lt;h3 id=&#34;visualizing-with-shiny&#34;&gt;Visualizing with Shiny&lt;/h3&gt;
&lt;p&gt;Beyond grinding students down with pure theory, I wrote an &lt;a href=&#34;https://ryansafner.shinyapps.io/ols_estimation_by_min_sse/&#34;&gt;interactive Shiny App&lt;/a&gt; with &lt;code&gt;R&lt;/code&gt; that demonstrates this process to choosing the optimal slope and intercept.&lt;/p&gt;
&lt;p&gt;With this example, I have a randomly-generated set of data (within specific parameters, to keep the graph scaled properly). I then allow users to choose a slope and intercept with sliders, to move the line accordingly. The graph also displays all of the errors as dashed red lines. Moving the slope or the intercept too much causes the generated line to create much larger errors. The graph also calculates the &lt;strong&gt;SSE&lt;/strong&gt; (which again, OLS minimizes), in addition to the standard error of the regression (&lt;strong&gt;SER&lt;/strong&gt;), which calculates the average size of the error.&lt;/p&gt;
&lt;p&gt;I would have loved to be able to graph the square of the errors, and display them, to show that OLS minimizes the area of those squares, if drawn. However, I was unable to figure out how to draw a square from each data point and the regression line (for squared error)| instead of a mere dashed line (for error).&lt;/p&gt;
&lt;p&gt;I love Shiny and have only begun to fully understand how it works and the benefits of its application. I have already coded several other models for intuitive visualization that I may post about later, namely a &lt;a href=&#34;https://ryansafner.shinyapps.io/ccmodel/&#34;&gt;Calculus of Consent model of the optimal voting rule&lt;/a&gt; in &lt;a href=&#34;https://ryansafner.com/courses/ECON410&#34;&gt;public choice&lt;/a&gt;, and ambitiously, the &lt;a href=&#34;https://ryansafner.shinyapps.io/consumer/&#34;&gt;consumer&amp;rsquo;s constrained optimization problem&lt;/a&gt; in &lt;a href=&#34;https://ryansafner.com/courses/ECON306&#34;&gt;intermediate microeconomics&lt;/a&gt;. Using sliders in shiny allow me to demonstrate the marginal effects of each parameter change on a model&amp;rsquo;s predicted outcome far better than just me proving several theoretical examples by way of equations. Students can instead see the change in real time.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Econometrics Lecture Slides on GitHub</title>
      <link>https://ryansafner.com/post/econometrics-slides-on-github/</link>
      <pubDate>Sat, 08 Sep 2018 06:57:00 +0000</pubDate>
      
      <guid>https://ryansafner.com/post/econometrics-slides-on-github/</guid>
      <description>&lt;p&gt;This semester, I am posting my lecture slides for my Econometrics class on &lt;a href=&#34;http://www.github.com/ryansafner/ECON480Fall2018&#34;&gt;GitHub&lt;/a&gt;. While I usually post my lecture slides for my courses on my website or link to my Dropbox, those are the final PDF documents. On GitHub, I am posting both the final PDFs as well as the source &lt;em&gt;.rmd&lt;/em&gt; files. I am primarily doing this for my econometrics students, who will be learning &lt;em&gt;R&lt;/em&gt; and &lt;em&gt;R Markdown&lt;/em&gt; for their assignments (which is how I write my slides), but this is also open to anyone.&lt;/p&gt;
&lt;p&gt;This brings two benefits. First, it provides an example of how to use &lt;em&gt;R Markdown&lt;/em&gt; and the benefits of writing plain-text files to convert into PDFs, html and other outputs (more on that in later posts). Second, it is also an example of using GitHub as a version control system for maintaining backups, a platform for collaboration, and exposure to industry tools of the software trade.&lt;/p&gt;
&lt;p&gt;Over the summer, I also converted to writing my research papers in &lt;em&gt;R Markdown&lt;/em&gt; and managing them on GitHub (and the same with this website). I have also attempted to begin collaborating with some of my coauthors via GitHub. They are presently in private repositories to keep research private until it is ready for publication, but I may consider making more public as examples.&lt;/p&gt;
&lt;p&gt;I have updated the Readme file on GitHub to provide more information for how to view, download, and read the &lt;em&gt;.rmd&lt;/em&gt; source files used to generate the slides. In future posts, I will discuss more about version control with GitHub, writing with &lt;em&gt;R Markdown&lt;/em&gt;, and managing workflow with these tools.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
